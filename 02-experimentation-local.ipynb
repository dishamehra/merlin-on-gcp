{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a92e9c9",
   "metadata": {},
   "source": [
    "# 02 - Experimentation - Local\n",
    "\n",
    "This notebook covers the following steps:\n",
    "\n",
    "1. Preparing the data using `NVTabular`.\n",
    "2. Train and evaluate the `TensorFlow` model.\n",
    "3. Export a `TensorFlow` model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16957e25",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4e89dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\n",
    "%env TF_MEMORY_ALLOCATION=0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a9d43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import nvtabular as nvt\n",
    "import tensorflow as tf\n",
    "\n",
    "from src.common import features, utils\n",
    "from src.data_preprocessing import etl\n",
    "from src.model_training import trainer\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "from google.protobuf.internal import api_implementation\n",
    "print(\"protobuf implementation type:\", api_implementation.Type())\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"GPU devices:\", tf.config.list_physical_devices(\"GPU\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ffed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'merlin-on-gcp'\n",
    "REGION = 'us-central1'\n",
    "BUCKET = 'merlin-on-gcp'\n",
    "\n",
    "MOVIES_CSV_DATASET_LOCATION = f\"gs://{BUCKET}/movielens25m/dataset/movies.csv\"\n",
    "RATINGS_CSV_DATASET_LOCATION = f\"gs://{BUCKET}/movielens25m/dataset/ratings.csv\"\n",
    "\n",
    "MODEL_DISPLAY_NAME = f'movielens25m-recommender'\n",
    "\n",
    "LOCAL_WORKSPACE = '_workspace'\n",
    "WORKSPACE = f\"gs://{BUCKET}/movielens25m\"\n",
    "EXPERIMENT_ARTIFACTS_DIR = os.path.join(WORKSPACE, 'experiments')\n",
    "\n",
    "TENSORBOARD_DISPLAY_NAME = f'tb-{PROJECT}'\n",
    "EXPERIMENT_NAME = f'{MODEL_DISPLAY_NAME}-experiment'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d234980",
   "metadata": {},
   "source": [
    "## Initialize Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1740b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_EXPERIMENT_ARTIFACTS = False\n",
    "if tf.io.gfile.exists(EXPERIMENT_ARTIFACTS_DIR) and REMOVE_EXPERIMENT_ARTIFACTS:\n",
    "    print(\"Removing previous experiment artifacts...\")\n",
    "    tf.io.gfile.rmtree(EXPERIMENT_ARTIFACTS_DIR)\n",
    "\n",
    "if not tf.io.gfile.exists(EXPERIMENT_ARTIFACTS_DIR):\n",
    "    print(\"Creating new experiment artifacts directory...\")\n",
    "    tf.io.gfile.mkdir(EXPERIMENT_ARTIFACTS_DIR)\n",
    "\n",
    "print(\"Preparing local workspace...\")\n",
    "if tf.io.gfile.exists(LOCAL_WORKSPACE):\n",
    "    tf.io.gfile.rmtree(LOCAL_WORKSPACE)\n",
    "tf.io.gfile.mkdir(LOCAL_WORKSPACE)\n",
    "    \n",
    "print(\"Workspace is ready.\")\n",
    "\n",
    "run_id = f\"run-local-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "EXPERIMENT_RUN_DIR = os.path.join(EXPERIMENT_ARTIFACTS_DIR, EXPERIMENT_NAME, run_id)\n",
    "print(\"Experiment run directory:\", EXPERIMENT_RUN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cea22f7",
   "metadata": {},
   "source": [
    "## 1. Preparing the data using NVTabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e55218",
   "metadata": {},
   "outputs": [],
   "source": [
    "ETL_OUTPUT_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'etl_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44834762",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train_dataset, transformed_test_dataset, transform_workflow = etl.run_etl( \n",
    "    MOVIES_CSV_DATASET_LOCATION, \n",
    "    RATINGS_CSV_DATASET_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbd8b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_test_dataset_dir = os.path.join(ETL_OUTPUT_DIR, \"transformed_data/test\")\n",
    "transformed_train_dataset_dir = os.path.join(ETL_OUTPUT_DIR, \"transformed_data/train\")\n",
    "local_transform_workflow_dir = os.path.join(LOCAL_WORKSPACE, 'transform_workflow')\n",
    "\n",
    "print(f\"Writting transformed training data to {transformed_train_dataset_dir}\")\n",
    "transformed_train_dataset.to_parquet(\n",
    "    output_path=transformed_train_dataset_dir,\n",
    "    shuffle=nvt.io.Shuffle.PER_PARTITION,\n",
    "    cats=features.CATEGORICAL_FEATURE_NAMES,\n",
    "    labels=features.TARGET_FEATURE_NAME,\n",
    "    dtypes=features.get_dtype_dict(),\n",
    ")\n",
    "print(\"Train data parquet files are written.\")\n",
    "\n",
    "print(f\"Writting transformed training data to {transformed_test_dataset_dir}\")\n",
    "transformed_test_dataset.to_parquet(\n",
    "    output_path=transformed_test_dataset_dir,\n",
    "    shuffle=False,\n",
    "    cats=features.CATEGORICAL_FEATURE_NAMES,\n",
    "    labels=features.TARGET_FEATURE_NAME,\n",
    "    dtypes=features.get_dtype_dict(),\n",
    ")\n",
    "print(\"Test data parquet files are written.\")\n",
    "\n",
    "logging.info(\"Saving transformation workflow...\")\n",
    "transform_workflow.save(local_transform_workflow_dir)\n",
    "logging.info(\"Transformation workflow is saved.\")\n",
    "\n",
    "print(\"Uploading trandorm workflow to Cloud Storage...\")\n",
    "utils.upload_directory(\n",
    "    local_transform_workflow_dir, \n",
    "    os.path.join(ETL_OUTPUT_DIR, 'transform_workflow')\n",
    ")\n",
    "try:\n",
    "    tf.io.gfile.rmtree(local_transform_workflow_dir)\n",
    "    tf.io.gfile.rmtree(\"categories\")\n",
    "except: pass\n",
    "print(\"Transformation uploaded to Cloud Storage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d34d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del transformed_train_dataset, transformed_test_dataset, transform_workflow\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119427cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {ETL_OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea626c6c",
   "metadata": {},
   "source": [
    "## 2. Train a TensorFlow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4fee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ec5c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_DATA_DIR = os.path.join(LOCAL_WORKSPACE, 'data')\n",
    "LOCAL_TRAIN_DATA_DIR = os.path.join(LOCAL_DATA_DIR, 'train')\n",
    "LOCAL_TEST_DATA_DIR = os.path.join(LOCAL_DATA_DIR, 'test')\n",
    "LOCAL_MODEL_DIR = os.path.join(LOCAL_WORKSPACE, 'exported_model')\n",
    "\n",
    "tf.io.gfile.mkdir(LOCAL_DATA_DIR)\n",
    "tf.io.gfile.mkdir(LOCAL_TRAIN_DATA_DIR)\n",
    "tf.io.gfile.mkdir(LOCAL_TEST_DATA_DIR)\n",
    "tf.io.gfile.mkdir(LOCAL_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aef2a9",
   "metadata": {},
   "source": [
    "### Prepare experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaa2460",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 1024 * 32,\n",
    "    'hidden_units': [128, 128],\n",
    "    'num_epochs': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6e06a0",
   "metadata": {},
   "source": [
    "### Download the data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135d13a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.copy_files(os.path.join(ETL_OUTPUT_DIR, 'transformed_data', 'train', '*.parquet'), LOCAL_TRAIN_DATA_DIR)\n",
    "utils.copy_files(os.path.join(ETL_OUTPUT_DIR, 'transformed_data', 'test', '*.parquet'), LOCAL_TEST_DATA_DIR)\n",
    "utils.download_directory(os.path.join(ETL_OUTPUT_DIR, 'transform_workflow'), LOCAL_WORKSPACE)\n",
    "print(\"Transformed data and transform workflow are downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733d9b04",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d39c3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nvt_workflow = nvt.Workflow.load(os.path.join(LOCAL_WORKSPACE, 'transform_workflow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4246c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_model = trainer.train(\n",
    "    train_data_file_pattern=os.path.join(LOCAL_TRAIN_DATA_DIR, '*.parquet'),\n",
    "    nvt_workflow=nvt_workflow,\n",
    "    hyperparams=hyperparams\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85d3f71",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e6e126",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loss, eval_mse = trainer.evaluate(\n",
    "    recommendation_model,\n",
    "    eval_data_file_pattern=os.path.join(LOCAL_TEST_DATA_DIR, '*.parquet'),\n",
    "    hyperparams=hyperparams\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b8554b",
   "metadata": {},
   "source": [
    "### Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091bda9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.export(\n",
    "    recommendation_model=recommendation_model,\n",
    "    nvt_workflow=nvt_workflow,\n",
    "    model_name=MODEL_DISPLAY_NAME,\n",
    "    export_dir=LOCAL_MODEL_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c5177e",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.upload_directory(LOCAL_MODEL_DIR, EXPORT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a91d059",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {EXPORT_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fbee52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m74"
  },
  "kernelspec": {
   "display_name": "nvt-11-0",
   "language": "python",
   "name": "nvt-11-0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

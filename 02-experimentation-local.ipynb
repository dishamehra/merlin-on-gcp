{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "628c7f80",
   "metadata": {},
   "source": [
    "# 02 - Experimentation - Local\n",
    "\n",
    "This notebook covers the following steps:\n",
    "\n",
    "1. Preparing the data using `NVTabular`.\n",
    "2. Train, and evaluate the `TensorFlow` model.\n",
    "3. Export a `TensorFlow` model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb6deaf",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74b9f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\n",
    "\n",
    "from google.protobuf.internal import api_implementation\n",
    "api_implementation._implementation_type = 'python'\n",
    "print(api_implementation.Type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d9d864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import cudf\n",
    "import nvtabular as nvt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from src.common import features, utils\n",
    "from src.data_preprocessing import etl\n",
    "from src.model_training import trainer\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1e3547",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'merlin-on-gcp'\n",
    "REGION = 'us-central1'\n",
    "BUCKET = 'merlin-on-gcp'\n",
    "VERTEX_SERVICE_ACCOUNT = f'vertex-sa-mlops@{PROJECT}.iam.gserviceaccount.com'\n",
    "\n",
    "MOVIES_CSV_DATASET_LOCATION = f\"gs://{BUCKET}/movielens25m/dataset/movies.csv\"\n",
    "RATINGS_CSV_DATASET_LOCATION = f\"gs://{BUCKET}/movielens25m/dataset/ratings.csv\"\n",
    "\n",
    "WORKSPACE = f\"gs://{BUCKET}/movielens25m\"\n",
    "EXPERIMENT_ARTIFACTS_DIR = os.path.join(WORKSPACE, 'experiments')\n",
    "\n",
    "TENSORBOARD_DISPLAY_NAME = f'tb-{PROJECT}'\n",
    "EXPERIMENT_NAME = f'{MODEL_DISPLAY_NAME}-experiment'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c97979",
   "metadata": {},
   "source": [
    "## Initialize Vertex AI Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d44215",
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_EXPERIMENT_ARTIFACTS = False\n",
    "if tf.io.gfile.exists(EXPERIMENT_ARTIFACTS_DIR) and REMOVE_EXPERIMENT_ARTIFACTS:\n",
    "    print(\"Removing previous experiment artifacts...\")\n",
    "    tf.io.gfile.rmtree(EXPERIMENT_ARTIFACTS_DIR)\n",
    "\n",
    "if not tf.io.gfile.exists(EXPERIMENT_ARTIFACTS_DIR):\n",
    "    print(\"Creating new experiment artifacts directory...\")\n",
    "    tf.io.gfile.mkdir(EXPERIMENT_ARTIFACTS_DIR)\n",
    "\n",
    "print(\"Workspace is ready.\")\n",
    "\n",
    "run_id = f\"run-local-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "EXPERIMENT_RUN_DIR = os.path.join(EXPERIMENT_ARTIFACTS_DIR, EXPERIMENT_NAME, run_id)\n",
    "print(\"Experiment run directory:\", EXPERIMENT_RUN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d4c0b1",
   "metadata": {},
   "source": [
    "## 1. Preparing the data using NVTabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81441d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ETL_OUTPUT_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'etl_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad923e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "etl.run_etl(\n",
    "    PROJECT, \n",
    "    REGION,  \n",
    "    MOVIES_CSV_DATASET_LOCATION, \n",
    "    RATINGS_CSV_DATASET_LOCATION, \n",
    "    ETL_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdcd8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {ETL_OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82bb324",
   "metadata": {},
   "source": [
    "## 2. Train a TensorFlow model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47811f4",
   "metadata": {},
   "source": [
    "### Prepare experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ce799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'logs')\n",
    "EXPORT_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7b7add",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_params = {\n",
    "    'train_data_file_pattern': os.path.join(ETL_OUTPUT_DIR, 'transformed_data', 'train', '*.parquet'),\n",
    "    'test_data_file_pattern': os.path.join(ETL_OUTPUT_DIR, 'transformed_data', 'test', '*.parquet'),\n",
    "    'transform_workflow_dir': os.path.join(ETL_OUTPUT_DIR, 'transform_workflow'),\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 1024 * 32,\n",
    "    'hidden_units': [128, 128],\n",
    "    'num_epochs': 1\n",
    "}\n",
    "\n",
    "vertex_ai.log_params(experiment_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6113647a",
   "metadata": {},
   "source": [
    "### Download the data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98958654",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.io.gfile.exists('data'):\n",
    "    tf.io.gfile.rmtree('data')\n",
    "if tf.io.gfile.exists('transform_workflow'):\n",
    "    tf.io.gfile.rmtree('transform_workflow')\n",
    "\n",
    "tf.io.gfile.mkdir('data')\n",
    "tf.io.gfile.mkdir('data/train')\n",
    "tf.io.gfile.mkdir('data/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5831d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.copy_files(experiment_params['train_data_files'], 'data/train')\n",
    "utils.copy_files(experiment_params['test_data_files'], 'data/test')\n",
    "utils.download_directory(experiment_params['transform_workflow_dir'], '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d45d4f4",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315c9947",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_model = trainer.train(\n",
    "    train_data_file_pattern='data/train/*.parquet',\n",
    "    nvt_workflow_dir='transform_workflow',\n",
    "    hyperparams=experiment_params,\n",
    "    log_dir=LOG_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7a0b03",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8a3666",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loss, eval_mse = trainer.evaluate(\n",
    "    recommendation_model,\n",
    "    eval_data_file_pattern='data/test/*.parquet',\n",
    "    hyperparams=hyperparams\n",
    ")\n",
    "\n",
    "vertex_ai.log_metrics({\"val_loss\": eval_loss, \"eval_mae\": eval_mae})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddf7dd9",
   "metadata": {},
   "source": [
    "### Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b822a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "\n",
    "imp.reload(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14453f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.export(\n",
    "    model=recommendation_model,\n",
    "    nvt_workflow=nvt.Workflow.load('transform_workflow'),\n",
    "    model_name=MODEL_DISPLAY_NAME,\n",
    "    export_dir=EXPORT_DIR\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m73",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m73"
  },
  "kernelspec": {
   "display_name": "nvt-cuda11.0",
   "language": "python",
   "name": "nvt-cuda11.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

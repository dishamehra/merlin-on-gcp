{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ea37b69",
   "metadata": {},
   "source": [
    "# 02 - Experimentation - Local\n",
    "\n",
    "This notebook covers the following steps:\n",
    "\n",
    "1. Preparing the data using `NVTabular`.\n",
    "2. Train, and evaluate the `TensorFlow` model.\n",
    "3. Export a `TensorFlow` model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fa19db",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15981693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\n",
      "env: TF_MEMORY_ALLOCATION=0.7\n"
     ]
    }
   ],
   "source": [
    "%env PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\n",
    "%env TF_MEMORY_ALLOCATION=0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60fe8196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protobuf implementation type: python\n",
      "TensorFlow: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import cudf\n",
    "import nvtabular as nvt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from src.common import features, utils\n",
    "from src.data_preprocessing import etl\n",
    "from src.model_training import trainer\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "from google.protobuf.internal import api_implementation\n",
    "print(\"protobuf implementation type:\", api_implementation.Type())\n",
    "print(\"TensorFlow:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "645aefa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'merlin-on-gcp'\n",
    "REGION = 'us-central1'\n",
    "BUCKET = 'merlin-on-gcp'\n",
    "VERTEX_SERVICE_ACCOUNT = f'vertex-sa-mlops@{PROJECT}.iam.gserviceaccount.com'\n",
    "\n",
    "MOVIES_CSV_DATASET_LOCATION = f\"gs://{BUCKET}/movielens25m/dataset/movies.csv\"\n",
    "RATINGS_CSV_DATASET_LOCATION = f\"gs://{BUCKET}/movielens25m/dataset/ratings.csv\"\n",
    "\n",
    "MODEL_DISPLAY_NAME = f'movielens25m-recommender'\n",
    "\n",
    "LOCAL_WORKSPACE = '_workspace'\n",
    "WORKSPACE = f\"gs://{BUCKET}/movielens25m\"\n",
    "EXPERIMENT_ARTIFACTS_DIR = os.path.join(WORKSPACE, 'experiments')\n",
    "\n",
    "TENSORBOARD_DISPLAY_NAME = f'tb-{PROJECT}'\n",
    "EXPERIMENT_NAME = f'{MODEL_DISPLAY_NAME}-experiment'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e3c059",
   "metadata": {},
   "source": [
    "## Initialize Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11b38dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing local workspace...\n",
      "Workspace is ready.\n",
      "Experiment run directory: gs://merlin-on-gcp/movielens25m/experiments/movielens25m-recommender-experiment/run-local-20210623161616\n"
     ]
    }
   ],
   "source": [
    "REMOVE_EXPERIMENT_ARTIFACTS = False\n",
    "if tf.io.gfile.exists(EXPERIMENT_ARTIFACTS_DIR) and REMOVE_EXPERIMENT_ARTIFACTS:\n",
    "    print(\"Removing previous experiment artifacts...\")\n",
    "    tf.io.gfile.rmtree(EXPERIMENT_ARTIFACTS_DIR)\n",
    "\n",
    "if not tf.io.gfile.exists(EXPERIMENT_ARTIFACTS_DIR):\n",
    "    print(\"Creating new experiment artifacts directory...\")\n",
    "    tf.io.gfile.mkdir(EXPERIMENT_ARTIFACTS_DIR)\n",
    "\n",
    "print(\"Preparing local workspace...\")\n",
    "if tf.io.gfile.exists(LOCAL_WORKSPACE):\n",
    "    tf.io.gfile.rmtree(LOCAL_WORKSPACE)\n",
    "tf.io.gfile.mkdir(LOCAL_WORKSPACE)\n",
    "    \n",
    "print(\"Workspace is ready.\")\n",
    "\n",
    "run_id = f\"run-local-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "EXPERIMENT_RUN_DIR = os.path.join(EXPERIMENT_ARTIFACTS_DIR, EXPERIMENT_NAME, run_id)\n",
    "print(\"Experiment run directory:\", EXPERIMENT_RUN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bcd0e4",
   "metadata": {},
   "source": [
    "## 1. Preparing the data using NVTabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d50e13da",
   "metadata": {},
   "outputs": [],
   "source": [
    "ETL_OUTPUT_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'etl_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e24ad04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading dataframes...\n",
      "INFO:root:Dataframe loaded.\n",
      "INFO:root:Movies data: (62423, 2).\n",
      "INFO:root:Ratings data: (25000095, 3).\n",
      "INFO:root:Splitting dataset to train and test splits...\n",
      "INFO:root:Train split size: 20000076\n",
      "INFO:root:Test split size: 5000019\n",
      "INFO:root:Loading NVTabular datasets...\n",
      "INFO:root:NVTabular datasets loaded.\n",
      "INFO:root:Creating transformation workflow...\n",
      "INFO:root:Fitting workflow to train data split...\n",
      "INFO:root:Transformation workflow is fitted.\n",
      "INFO:root:Transforming train dataset...\n",
      "INFO:root:Train data is transformed.\n",
      "INFO:root:Transforming test dataset...\n",
      "INFO:root:Test dataset is transformed.\n",
      "INFO:root:Saving transformation workflow...\n",
      "INFO:root:Transformation workflow is saved.\n",
      "INFO:root:Uploading trandorm workflow to Cloud Storage...\n",
      "INFO:root:Transformation uploaded to Cloud Storage.\n"
     ]
    }
   ],
   "source": [
    "etl.run_etl(\n",
    "    PROJECT, \n",
    "    REGION,  \n",
    "    MOVIES_CSV_DATASET_LOCATION, \n",
    "    RATINGS_CSV_DATASET_LOCATION, \n",
    "    ETL_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf146cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://merlin-on-gcp/movielens25m/experiments/movielens25m-recommender-experiment/run-local-20210623161616/etl_output/transform_workflow/\n",
      "gs://merlin-on-gcp/movielens25m/experiments/movielens25m-recommender-experiment/run-local-20210623161616/etl_output/transformed_data/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls {ETL_OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc14987",
   "metadata": {},
   "source": [
    "## 2. Train a TensorFlow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4c9309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "041d14d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_DATA_DIR = os.path.join(LOCAL_WORKSPACE, 'data')\n",
    "LOCAL_TRAIN_DATA_DIR = os.path.join(LOCAL_DATA_DIR, 'train')\n",
    "LOCAL_TEST_DATA_DIR = os.path.join(LOCAL_DATA_DIR, 'test')\n",
    "LOCAL_MODEL_DIR = os.path.join(LOCAL_WORKSPACE, 'exported_model')\n",
    "\n",
    "tf.io.gfile.mkdir(LOCAL_DATA_DIR)\n",
    "tf.io.gfile.mkdir(LOCAL_TRAIN_DATA_DIR)\n",
    "tf.io.gfile.mkdir(LOCAL_TEST_DATA_DIR)\n",
    "tf.io.gfile.mkdir(LOCAL_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36619d7c",
   "metadata": {},
   "source": [
    "### Prepare experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c4f9572",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 1024 * 32,\n",
    "    'hidden_units': [128, 128],\n",
    "    'num_epochs': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973c4c0d",
   "metadata": {},
   "source": [
    "### Download the data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3709b855",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.copy_files(os.path.join(ETL_OUTPUT_DIR, 'transformed_data', 'train', '*.parquet'), LOCAL_TRAIN_DATA_DIR)\n",
    "utils.copy_files(os.path.join(ETL_OUTPUT_DIR, 'transformed_data', 'test', '*.parquet'), LOCAL_TEST_DATA_DIR)\n",
    "utils.download_directory(os.path.join(ETL_OUTPUT_DIR, 'transform_workflow'), LOCAL_WORKSPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e48123",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4addba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "nvt_workflow = nvt.Workflow.load(os.path.join(LOCAL_WORKSPACE, 'transform_workflow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34ce35a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Hyperparameter:\n",
      "INFO:root:{'learning_rate': 0.001, 'batch_size': 32768, 'hidden_units': [128, 128], 'num_epochs': 1}\n",
      "INFO:root:\n",
      "INFO:root:Preparing train dataset loader...\n",
      "INFO:root:Embedding shapes: {'userId': (162542, 512), 'movieId': (56586, 512), 'genres': (21, 16)}\n",
      "INFO:root:Compiling the model...\n",
      "INFO:root:Model fitting started...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "611/611 [==============================] - 22s 33ms/step - loss: 0.5418 - mae: 0.3659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Model fitting finished.\n"
     ]
    }
   ],
   "source": [
    "recommendation_model = trainer.train(\n",
    "    train_data_file_pattern=os.path.join(LOCAL_TRAIN_DATA_DIR, '*.parquet'),\n",
    "    nvt_workflow=nvt_workflow,\n",
    "    hyperparams=hyperparams\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b374a5e",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08224e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Preparing evaluation dataset loader...\n",
      "INFO:root:Evaluating the model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153/153 [==============================] - 2s 10ms/step - loss: 0.4888 - mae: 0.3226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Evaluation loss: 0.4887738823890686 - Evaluation MAE 0.3225647211074829\n"
     ]
    }
   ],
   "source": [
    "eval_loss, eval_mse = trainer.evaluate(\n",
    "    recommendation_model,\n",
    "    eval_data_file_pattern=os.path.join(LOCAL_TEST_DATA_DIR, '*.parquet'),\n",
    "    hyperparams=hyperparams\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90b754d",
   "metadata": {},
   "source": [
    "### Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e966814b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: _workspace/exported_model/movielens25m-recommender_tf/1/model.savedmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: _workspace/exported_model/movielens25m-recommender_tf/1/model.savedmodel/assets\n"
     ]
    }
   ],
   "source": [
    "trainer.export(\n",
    "    recommendation_model=recommendation_model,\n",
    "    nvt_workflow=nvt_workflow,\n",
    "    model_name=MODEL_DISPLAY_NAME,\n",
    "    export_dir=LOCAL_MODEL_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e4d56b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.upload_directory(LOCAL_MODEL_DIR, EXPORT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc56c97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://merlin-on-gcp/movielens25m/experiments/movielens25m-recommender-experiment/run-local-20210623161616/model/movielens25m-recommender/\n",
      "gs://merlin-on-gcp/movielens25m/experiments/movielens25m-recommender-experiment/run-local-20210623161616/model/movielens25m-recommender_nvt/\n",
      "gs://merlin-on-gcp/movielens25m/experiments/movielens25m-recommender-experiment/run-local-20210623161616/model/movielens25m-recommender_tf/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls {EXPORT_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806e2768",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m73",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m73"
  },
  "kernelspec": {
   "display_name": "nvt-11-0",
   "language": "python",
   "name": "nvt-11-0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

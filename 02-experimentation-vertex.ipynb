{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d184142a",
   "metadata": {},
   "source": [
    "# 02 - Experimentation\n",
    "\n",
    "This notebook covers the following steps:\n",
    "\n",
    "1. Preparing the data using `NVTabular` locally.\n",
    "2. Train, evaluate, and export a `TensorFlow` model locally.\n",
    "3. Build the Docker container image.\n",
    "4. Submit a `Vertex AI` custom job to prepare the data.\n",
    "5. Submit a `Vertex AI` custom job to train and export the model.\n",
    "6. Upload the exported model as a Vertex AI model resource."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fad9e8",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1f2329",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\n",
    "\n",
    "from google.protobuf.internal import api_implementation\n",
    "api_implementation._implementation_type = 'python'\n",
    "print(api_implementation.Type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58079cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import cudf\n",
    "import nvtabular as nvt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from src.common import features, utils\n",
    "from src.data_preprocessing import etl\n",
    "from src.model_training import trainer\n",
    "\n",
    "#from google.cloud import aiplatform as vertex_ai\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50930867",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'merlin-on-gcp'\n",
    "REGION = 'us-central1'\n",
    "BUCKET = 'merlin-on-gcp'\n",
    "VERTEX_SERVICE_ACCOUNT = f'vertex-sa-mlops@{PROJECT}.iam.gserviceaccount.com'\n",
    "\n",
    "# MOVIES_DATASET_DISPLAY_NAME = 'movielens25m-movies'\n",
    "# RATINGS_DATASET_DISPLAY_NAME = 'movielens25m-ratings'\n",
    "\n",
    "MOVIES_CSV_DATASET_LOCATION = f\"gs://{BUCKET}/movielens25m/dataset/movies.csv\"\n",
    "RATINGS_CSV_DATASET_LOCATION = f\"gs://{BUCKET}/movielens25m/dataset/ratings.csv\"\n",
    "\n",
    "# MODEL_DISPLAY_NAME = f'movielens25m-recommender'\n",
    "\n",
    "WORKSPACE = f\"gs://{BUCKET}/movielens25m\"\n",
    "EXPERIMENT_ARTIFACTS_DIR = os.path.join(WORKSPACE, 'experiments')\n",
    "\n",
    "TENSORBOARD_DISPLAY_NAME = f'tb-{PROJECT}'\n",
    "EXPERIMENT_NAME = f'{MODEL_DISPLAY_NAME}-experiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a78720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vertex_ai.init(\n",
    "#     project=PROJECT,\n",
    "#     location=REGION,\n",
    "#     staging_bucket=BUCKET,\n",
    "#     experiment=EXPERIMENT_NAME,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0469c2bf",
   "metadata": {},
   "source": [
    "## Create Vertex TensorBoard Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa1c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gcloud beta ai tensorboards create --display-name={TENSORBOARD_DISPLAY_NAME} \\\n",
    "#   --project={PROJECT} --region={REGION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c18a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TENSORBOARD_RESOURCE_NAME = \"projects/659831510405/locations/us-central1/tensorboards/4450717516120981504\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e74f51",
   "metadata": {},
   "source": [
    "## Initialize Vertex AI Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7695457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_EXPERIMENT_ARTIFACTS = False\n",
    "if tf.io.gfile.exists(EXPERIMENT_ARTIFACTS_DIR) and REMOVE_EXPERIMENT_ARTIFACTS:\n",
    "    print(\"Removing previous experiment artifacts...\")\n",
    "    tf.io.gfile.rmtree(EXPERIMENT_ARTIFACTS_DIR)\n",
    "\n",
    "if not tf.io.gfile.exists(EXPERIMENT_ARTIFACTS_DIR):\n",
    "    print(\"Creating new experiment artifacts directory...\")\n",
    "    tf.io.gfile.mkdir(EXPERIMENT_ARTIFACTS_DIR)\n",
    "\n",
    "print(\"Workspace is ready.\")\n",
    "\n",
    "run_id = f\"run-local-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "# vertex_ai.start_run(run_id)\n",
    "\n",
    "EXPERIMENT_RUN_DIR = os.path.join(EXPERIMENT_ARTIFACTS_DIR, EXPERIMENT_NAME, run_id)\n",
    "print(\"Experiment run directory:\", EXPERIMENT_RUN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882b21a3",
   "metadata": {},
   "source": [
    "## 1. Preparing the data using NVTabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc968633",
   "metadata": {},
   "outputs": [],
   "source": [
    "ETL_OUTPUT_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'etl_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c7d721",
   "metadata": {},
   "outputs": [],
   "source": [
    "etl.run_etl(\n",
    "    PROJECT, \n",
    "    REGION, \n",
    "#     MOVIES_DATASET_DISPLAY_NAME, \n",
    "#     RATINGS_DATASET_DISPLAY_NAME, \n",
    "    MOVIES_CSV_DATASET_LOCATION, \n",
    "    RATINGS_CSV_DATASET_LOCATION, \n",
    "    ETL_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4ccbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {ETL_OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4107b098",
   "metadata": {},
   "source": [
    "## 2. Train a TensorFlow model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facd61d8",
   "metadata": {},
   "source": [
    "### Prepare experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2b30e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'logs')\n",
    "EXPORT_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4116e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_params = {\n",
    "    'train_data_file_pattern': os.path.join(ETL_OUTPUT_DIR, 'transformed_data', 'train', '*.parquet'),\n",
    "    'test_data_file_pattern': os.path.join(ETL_OUTPUT_DIR, 'transformed_data', 'test', '*.parquet'),\n",
    "    'transform_workflow_dir': os.path.join(ETL_OUTPUT_DIR, 'transform_workflow'),\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 1024 * 32,\n",
    "    'hidden_units': [128, 128],\n",
    "    'num_epochs': 1\n",
    "}\n",
    "\n",
    "vertex_ai.log_params(experiment_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a184bd",
   "metadata": {},
   "source": [
    "### Download the data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340fd708",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.io.gfile.exists('data'):\n",
    "    tf.io.gfile.rmtree('data')\n",
    "if tf.io.gfile.exists('transform_workflow'):\n",
    "    tf.io.gfile.rmtree('transform_workflow')\n",
    "\n",
    "tf.io.gfile.mkdir('data')\n",
    "tf.io.gfile.mkdir('data/train')\n",
    "tf.io.gfile.mkdir('data/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e7e921",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.copy_files(experiment_params['train_data_files'], 'data/train')\n",
    "utils.copy_files(experiment_params['test_data_files'], 'data/test')\n",
    "utils.download_directory(experiment_params['transform_workflow_dir'], '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f000ddef",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185771e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_model = trainer.train(\n",
    "    train_data_file_pattern='data/train/*.parquet',\n",
    "    nvt_workflow_dir='transform_workflow',\n",
    "    hyperparams=experiment_params,\n",
    "    log_dir=LOG_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d830aa6e",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600e2868",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loss, eval_mse = trainer.evaluate(\n",
    "    recommendation_model,\n",
    "    eval_data_file_pattern='data/test/*.parquet',\n",
    "    hyperparams=hyperparams\n",
    ")\n",
    "\n",
    "vertex_ai.log_metrics({\"val_loss\": eval_loss, \"eval_mae\": eval_mae})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454fe624",
   "metadata": {},
   "source": [
    "### Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889ba4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "\n",
    "imp.reload(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4aaf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.export(\n",
    "    model=recommendation_model,\n",
    "    nvt_workflow=nvt.Workflow.load('transform_workflow'),\n",
    "    model_name=MODEL_DISPLAY_NAME,\n",
    "    export_dir=EXPORT_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a98c0e",
   "metadata": {},
   "source": [
    "## 3. Build training container image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52af816",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME=\"nvt-cuda11.0-tf2.4\"\n",
    "IMAGE_URI=f\"gcr.io/{PROJECT}/{IMAGE_NAME}\"\n",
    "print(IMAGE_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d7e142",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud builds submit --tag {IMAGE_URI} . --timeout=30m --machine-type=e2-highcpu-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bc3a8d",
   "metadata": {},
   "source": [
    "## 4. Submit Vertex AI Custom Job for ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194de538",
   "metadata": {},
   "source": [
    "### Prepare worker pool specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1717d284",
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_pool_specs =  [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\",\n",
    "            \"accelerator_type\": \"NVIDIA_TESLA_V100\",\n",
    "            \"accelerator_count\": 1,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": IMAGE_URI,\n",
    "            \"command\": [\"python\", \"src/data_preprocessing/task.py\"],\n",
    "            \"args\": [\n",
    "                f'--project={PROJECT}', \n",
    "                f'--region={REGION}',\n",
    "                f'--movies-dataset-display=name='{MOVIES_DATASET_DISPLAY_NAME}',\n",
    "                f'--ratings-dataset-display=name='{RATINGS_DATASET_DISPLAY_NAME}',\n",
    "                f'--etl-output-dir='{ETL_OUTPUT_DIR}',\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c45de06",
   "metadata": {},
   "source": [
    "\n",
    "### Submit and monitor the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e02ffcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = \"movielens-nvt-etl-{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "job = vertex_ai.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    ")\n",
    "\n",
    "job.run(\n",
    "    sync=True, \n",
    "    service_account=VERTEX_SERVICE_ACCOUNT,\n",
    "    tensorboard=TENSORBOARD_RESOURCE_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95f30d4",
   "metadata": {},
   "source": [
    "## 5. Submit Vertex AI Custom Job for Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70c9235",
   "metadata": {},
   "source": [
    "### Prepare worker pool specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5daa467",
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_pool_specs =  [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\",\n",
    "            \"accelerator_type\": \"NVIDIA_TESLA_V100\",\n",
    "            \"accelerator_count\": 1,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": IMAGE_URI,\n",
    "            \"command\": [\"python\", \"src/data_preprocessing/task.py\"],\n",
    "            \"args\": [\n",
    "                f'--project={PROJECT}', \n",
    "                f'--region={REGION}',\n",
    "                f'--movies-dataset-display=name='{MOVIES_DATASET_DISPLAY_NAME}',\n",
    "                f'--ratings-dataset-display=name='{RATINGS_DATASET_DISPLAY_NAME}',\n",
    "                f'--etl-output-dir='{ETL_OUTPUT_DIR}',\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4a3944",
   "metadata": {},
   "source": [
    "### Submit and monitor the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c733bdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = \"movielens-tf-training-{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "job = vertex_ai.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    ")\n",
    "\n",
    "job.run(\n",
    "    sync=True, \n",
    "    service_account=VERTEX_SERVICE_ACCOUNT,\n",
    "    tensorboard=TENSORBOARD_RESOURCE_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78fd09e",
   "metadata": {},
   "source": [
    "## 6. Upload the model Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8ee154",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRITON_SERVER_IMAGE = f\"gcr.io/{PROJECT}/triton-{MODEL_DISPLAY_NAME}:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6928036d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.Model.upload(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    artifact_uri=EXPORT_DIR,\n",
    "    serving_container_image_uri=TRITON_SERVER_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d5ea1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e86fd02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m73",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m73"
  },
  "kernelspec": {
   "display_name": "nvt-cuda11.0",
   "language": "python",
   "name": "nvt-cuda11.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

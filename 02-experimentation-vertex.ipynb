{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c8c97e2",
   "metadata": {},
   "source": [
    "# 02 - Experimentation - Vertex AI\n",
    "\n",
    "This notebook covers the following steps:\n",
    "\n",
    "1. Build the Docker container image.\n",
    "2. Submit a `Vertex AI` custom job to prepare the data.\n",
    "3. Submit a `Vertex AI` custom job to train and export the model.\n",
    "4. Upload the exported model as a Vertex AI model resource."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91110263",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "890effd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81e3f9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'merlin-on-gcp'\n",
    "REGION = 'us-central1'\n",
    "BUCKET = 'merlin-on-gcp'\n",
    "VERTEX_SERVICE_ACCOUNT = f'vertex-sa-mlops@{PROJECT}.iam.gserviceaccount.com'\n",
    "\n",
    "MOVIES_DATASET_DISPLAY_NAME = 'movielens25m-movies'\n",
    "RATINGS_DATASET_DISPLAY_NAME = 'movielens25m-ratings'\n",
    "\n",
    "MODEL_DISPLAY_NAME = f'movielens25m-recommender'\n",
    "\n",
    "WORKSPACE = f\"gs://{BUCKET}/movielens25m\"\n",
    "EXPERIMENT_ARTIFACTS_DIR = os.path.join(WORKSPACE, 'experiments')\n",
    "\n",
    "TENSORBOARD_DISPLAY_NAME = f'tb-{PROJECT}'\n",
    "EXPERIMENT_NAME = f'{MODEL_DISPLAY_NAME}-experiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48439a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=BUCKET,\n",
    "    experiment=EXPERIMENT_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5b622c",
   "metadata": {},
   "source": [
    "## Create Vertex TensorBoard Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c209733",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud beta ai tensorboards create --display-name={TENSORBOARD_DISPLAY_NAME} \\\n",
    "  --project={PROJECT} --region={REGION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95978e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSORBOARD_RESOURCE_NAME = \"projects/659831510405/locations/us-central1/tensorboards/4450717516120981504\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa550d9",
   "metadata": {},
   "source": [
    "## Initialize Vertex AI Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f3123de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace is ready.\n",
      "INFO:root:Resource movielens25m-recommender-experiment-run-local-20210703203144 not found.\n",
      "INFO:root:Creating Resource movielens25m-recommender-experiment-run-local-20210703203144\n",
      "INFO:root:Resource movielens25m-recommender-experiment-run-local-20210703203144-metrics not found.\n",
      "INFO:root:Creating Resource movielens25m-recommender-experiment-run-local-20210703203144-metrics\n",
      "Experiment run directory: gs://merlin-on-gcp/movielens25m/experiments/movielens25m-recommender-experiment/run-local-20210703203144\n"
     ]
    }
   ],
   "source": [
    "REMOVE_EXPERIMENT_ARTIFACTS = False\n",
    "if tf.io.gfile.exists(EXPERIMENT_ARTIFACTS_DIR) and REMOVE_EXPERIMENT_ARTIFACTS:\n",
    "    print(\"Removing previous experiment artifacts...\")\n",
    "    tf.io.gfile.rmtree(EXPERIMENT_ARTIFACTS_DIR)\n",
    "\n",
    "if not tf.io.gfile.exists(EXPERIMENT_ARTIFACTS_DIR):\n",
    "    print(\"Creating new experiment artifacts directory...\")\n",
    "    tf.io.gfile.mkdir(EXPERIMENT_ARTIFACTS_DIR)\n",
    "\n",
    "print(\"Workspace is ready.\")\n",
    "\n",
    "run_id = f\"run-local-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "vertex_ai.start_run(run_id)\n",
    "\n",
    "EXPERIMENT_RUN_DIR = os.path.join(EXPERIMENT_ARTIFACTS_DIR, EXPERIMENT_NAME, run_id)\n",
    "print(\"Experiment run directory:\", EXPERIMENT_RUN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e949f531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_gcs_location(dataset_display_name):\n",
    "    datasets = vertex_ai.TabularDataset.list()\n",
    "    dataset = None\n",
    "    for entry in datasets:\n",
    "        if entry.display_name == dataset_display_name:\n",
    "            dataset = entry\n",
    "            break\n",
    "\n",
    "    if not dataset:\n",
    "        raise ValueError(f\"Dataset with display name {dataset_display_name} does not exist!\")\n",
    "    \n",
    "    return dataset.gca_resource.metadata['inputConfig']['gcsSource']['uri'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d3af1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies CSV dataset location: gs://merlin-on-gcp/movielens25m/dataset/ratings.csv\n",
      "Ratings CSV dataset location: gs://merlin-on-gcp/movielens25m/dataset/ratings.csv\n"
     ]
    }
   ],
   "source": [
    "movies_csv_dataset_location = get_dataset_gcs_location(MOVIES_DATASET_DISPLAY_NAME)\n",
    "ratings_csv_dataset_location = get_dataset_gcs_location(RATINGS_DATASET_DISPLAY_NAME)\n",
    "print(\"Movies CSV dataset location:\", ratings_csv_dataset_location)\n",
    "print(\"Ratings CSV dataset location:\", ratings_csv_dataset_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bac1ba0",
   "metadata": {},
   "source": [
    "## 3. Build training container image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b36e8c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcr.io/merlin-on-gcp/movielens-nvt0.3-tf2.4\n"
     ]
    }
   ],
   "source": [
    "IMAGE_NAME=\"movielens-nvt0.3-tf2.4\"\n",
    "IMAGE_URI=f\"gcr.io/{PROJECT}/{IMAGE_NAME}\"\n",
    "print(IMAGE_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737d2f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud builds submit --tag {IMAGE_URI} . --timeout=45m --machine-type=e2-highcpu-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdb7ed7",
   "metadata": {},
   "source": [
    "## 4. Submit Vertex AI Custom Job for ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68d1ba2",
   "metadata": {},
   "source": [
    "### Prepare worker pool specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aac2e01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ETL_OUTPUT_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'etl_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de6c17e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_pool_specs =  [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\",\n",
    "            \"accelerator_type\": \"NVIDIA_TESLA_V100\",\n",
    "            \"accelerator_count\": 1,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": IMAGE_URI,\n",
    "            #\"command\": [\"python\", \"-m\", \"src.data_preprocessing.task\"],\n",
    "            \"args\": [\n",
    "                \"python\",\n",
    "                \"-m\",\n",
    "                \"src.data_preprocessing.task\",\n",
    "                f'--movies-csv-data-location={movies_csv_dataset_location}',\n",
    "                f'--ratings-csv-data-location={ratings_csv_dataset_location}',\n",
    "                f'--etl-output-dir={ETL_OUTPUT_DIR}',\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738ca2a8",
   "metadata": {},
   "source": [
    "\n",
    "### Submit and monitor the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cee1317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:Creating CustomJob\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob created. Resource name: projects/659831510405/locations/us-central1/customJobs/6576895927309565952\n",
      "INFO:google.cloud.aiplatform.jobs:To use this CustomJob in another session:\n",
      "INFO:google.cloud.aiplatform.jobs:custom_job = aiplatform.CustomJob.get('projects/659831510405/locations/us-central1/customJobs/6576895927309565952')\n",
      "INFO:google.cloud.aiplatform.jobs:View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/6576895927309565952?project=659831510405\n",
      "INFO:google.cloud.aiplatform.jobs:View Tensorboard:\n",
      "https://us-central1.tensorboard.googleusercontent.com/experiment/projects+659831510405+locations+us-central1+tensorboards+4450717516120981504+experiments+6576895927309565952\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/659831510405/locations/us-central1/customJobs/6576895927309565952 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/659831510405/locations/us-central1/customJobs/6576895927309565952 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/659831510405/locations/us-central1/customJobs/6576895927309565952 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "job_name = \"movielens-nvt-etl-{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "job = vertex_ai.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=os.path.join(WORKSPACE, 'jobs')\n",
    ")\n",
    "\n",
    "job.run(\n",
    "    sync=True, \n",
    "    service_account=VERTEX_SERVICE_ACCOUNT,\n",
    "    tensorboard=TENSORBOARD_RESOURCE_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6744498",
   "metadata": {},
   "source": [
    "## 5. Submit Vertex AI Custom Job for Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7ac86e",
   "metadata": {},
   "source": [
    "### Prepare worker pool specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dde65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORM_TRAIN_DATA = os.path.join(ETL_OUTPUT_DIR, 'transformed_data/train/*.parquet')\n",
    "TRANSFORM_EVAL_DATA = os.path.join(ETL_OUTPUT_DIR, 'transformed_data/test/*.parquet')\n",
    "TRANSFORM_WORKFLOW_DIR = os.path.join(ETL_OUTPUT_DIR, 'transform_workflow')\n",
    "NUM_EPOCHS = 1\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aa99f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_pool_specs =  [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\",\n",
    "            \"accelerator_type\": \"NVIDIA_TESLA_V100\",\n",
    "            \"accelerator_count\": 1,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": IMAGE_URI,\n",
    "            #\"command\": [\"python\", \"-m\", \"src.model_training.task\"],\n",
    "            \"args\": [\n",
    "                \"python\",\n",
    "                \"-m\",\n",
    "                \"src.model_training.task\",\n",
    "                f'--train-data-file-pattern={TRANSFORM_TRAIN_DATA}',\n",
    "                f'--eval-data-file-pattern={TRANSFORM_EVAL_DATA}',\n",
    "                f'--nvt-workflow-dir={TRANSFORM_WORKFLOW_DIR}',\n",
    "                f'--num-epochs={NUM_EPOCHS}',\n",
    "                f'--learning-rate={LEARNING_RATE}',\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db816143",
   "metadata": {},
   "source": [
    "### Submit and monitor the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a17ddfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = \"movielens-tf-training-{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "job = vertex_ai.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    ")\n",
    "\n",
    "job.run(\n",
    "    sync=True, \n",
    "    service_account=VERTEX_SERVICE_ACCOUNT,\n",
    "    tensorboard=TENSORBOARD_RESOURCE_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31c65b3",
   "metadata": {},
   "source": [
    "## 6. Upload the model Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed7b3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRITON_SERVER_IMAGE = f\"gcr.io/{PROJECT}/triton-{MODEL_DISPLAY_NAME}:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ac6ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.Model.upload(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    artifact_uri=EXPORT_DIR,\n",
    "    serving_container_image_uri=TRITON_SERVER_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912a3edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daa4e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

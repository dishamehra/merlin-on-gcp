{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31811b9d",
   "metadata": {},
   "source": [
    "# 02 - Experimentation - Vertex AI\n",
    "\n",
    "This notebook covers the following steps:\n",
    "\n",
    "1. Build the Docker container image.\n",
    "2. Submit a `Vertex AI` custom job to prepare the data.\n",
    "3. Submit a `Vertex AI` custom job to train and export the model.\n",
    "4. Upload the exported model as a Vertex AI model resource."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a973898",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da783be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635b70ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'merlin-on-gcp'\n",
    "REGION = 'us-central1'\n",
    "BUCKET = 'merlin-on-gcp'\n",
    "VERTEX_SERVICE_ACCOUNT = f'vertex-sa-mlops@{PROJECT}.iam.gserviceaccount.com'\n",
    "\n",
    "MOVIES_DATASET_DISPLAY_NAME = 'movielens25m-movies'\n",
    "RATINGS_DATASET_DISPLAY_NAME = 'movielens25m-ratings'\n",
    "\n",
    "MODEL_DISPLAY_NAME = f'movielens25m-recommender'\n",
    "\n",
    "WORKSPACE = f\"gs://{BUCKET}/movielens25m\"\n",
    "EXPERIMENT_ARTIFACTS_DIR = os.path.join(WORKSPACE, 'experiments')\n",
    "\n",
    "TENSORBOARD_DISPLAY_NAME = f'tb-{PROJECT}'\n",
    "EXPERIMENT_NAME = f'{MODEL_DISPLAY_NAME}-experiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa56752",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=BUCKET,\n",
    "    experiment=EXPERIMENT_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2ffcdf",
   "metadata": {},
   "source": [
    "## Create Vertex TensorBoard Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e4ca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud beta ai tensorboards create --display-name={TENSORBOARD_DISPLAY_NAME} \\\n",
    "  --project={PROJECT} --region={REGION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064cd5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSORBOARD_RESOURCE_NAME = \"projects/659831510405/locations/us-central1/tensorboards/4450717516120981504\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea154f7f",
   "metadata": {},
   "source": [
    "## Initialize Vertex AI Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e259193",
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_EXPERIMENT_ARTIFACTS = False\n",
    "if tf.io.gfile.exists(EXPERIMENT_ARTIFACTS_DIR) and REMOVE_EXPERIMENT_ARTIFACTS:\n",
    "    print(\"Removing previous experiment artifacts...\")\n",
    "    tf.io.gfile.rmtree(EXPERIMENT_ARTIFACTS_DIR)\n",
    "\n",
    "if not tf.io.gfile.exists(EXPERIMENT_ARTIFACTS_DIR):\n",
    "    print(\"Creating new experiment artifacts directory...\")\n",
    "    tf.io.gfile.mkdir(EXPERIMENT_ARTIFACTS_DIR)\n",
    "\n",
    "print(\"Workspace is ready.\")\n",
    "\n",
    "run_id = f\"run-local-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "vertex_ai.start_run(run_id)\n",
    "\n",
    "EXPERIMENT_RUN_DIR = os.path.join(EXPERIMENT_ARTIFACTS_DIR, EXPERIMENT_NAME, run_id)\n",
    "print(\"Experiment run directory:\", EXPERIMENT_RUN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374cc2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_gcs_location(dataset_display_name):\n",
    "    datasets = vertex_ai.TabularDataset.list()\n",
    "    dataset = None\n",
    "    for entry in datasets:\n",
    "        if entry.display_name == dataset_display_name:\n",
    "            dataset = entry\n",
    "            break\n",
    "\n",
    "    if not dataset:\n",
    "        raise ValueError(f\"Dataset with display name {dataset_display_name} does not exist!\")\n",
    "    \n",
    "    return dataset.gca_resource.metadata['inputConfig']['gcsSource']['uri'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a880c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_csv_dataset_location = get_dataset_gcs_location(MOVIES_DATASET_DISPLAY_NAME)\n",
    "ratings_csv_dataset_location = get_dataset_gcs_location(RATINGS_DATASET_DISPLAY_NAME)\n",
    "print(\"Movies CSV dataset location:\", ratings_csv_dataset_location)\n",
    "print(\"Ratings CSV dataset location:\", ratings_csv_dataset_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86b9a0c",
   "metadata": {},
   "source": [
    "## 3. Build training container image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7064b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME=\"movielens-nvt0.3-tf2.4\"\n",
    "IMAGE_URI=f\"gcr.io/{PROJECT}/{IMAGE_NAME}\"\n",
    "print(IMAGE_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a96080",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud builds submit --tag {IMAGE_URI} . --timeout=45m --machine-type=e2-highcpu-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a602e2",
   "metadata": {},
   "source": [
    "## 4. Submit Vertex AI Custom Job for ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c19c44",
   "metadata": {},
   "source": [
    "### Prepare worker pool specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d17dc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ETL_OUTPUT_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'etl_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e492f173",
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_pool_specs =  [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\",\n",
    "            \"accelerator_type\": \"NVIDIA_TESLA_V100\",\n",
    "            \"accelerator_count\": 1,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": IMAGE_URI,\n",
    "            \"command\": [\"python\", \"-m\", \"src.data_preprocessing.task\"],\n",
    "            \"args\": [\n",
    "                f'--movies-csv-dataset-location={movies_csv_dataset_location}',\n",
    "                f'--ratings-csv-dataset-location={ratings_csv_dataset_location}',\n",
    "                f'--etl-output-dir={ETL_OUTPUT_DIR}',\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf8c851",
   "metadata": {},
   "source": [
    "\n",
    "### Submit and monitor the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adc3832",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = \"movielens-nvt-etl-{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "job = vertex_ai.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=os.path.join(WORKSPACE, 'jobs')\n",
    ")\n",
    "\n",
    "job.run(\n",
    "    sync=True, \n",
    "    service_account=VERTEX_SERVICE_ACCOUNT,\n",
    "    tensorboard=TENSORBOARD_RESOURCE_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe18aceb",
   "metadata": {},
   "source": [
    "## 5. Submit Vertex AI Custom Job for Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ad8355",
   "metadata": {},
   "source": [
    "### Prepare worker pool specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2006ed2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORM_TRAIN_DATA = os.path.join(ETL_OUTPUT_DIR, 'transformed_data/train/*.parquet')\n",
    "TRANSFORM_EVAL_DATA = os.path.join(ETL_OUTPUT_DIR, 'transformed_data/test/*.parquet')\n",
    "TRANSFORM_WORKFLOW_DIR = os.path.join(ETL_OUTPUT_DIR, 'transform_workflow')\n",
    "NUM_EPOCHS = 1\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6658522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_pool_specs =  [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\",\n",
    "            \"accelerator_type\": \"NVIDIA_TESLA_V100\",\n",
    "            \"accelerator_count\": 1,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": IMAGE_URI,\n",
    "            \"command\": [\"python\", \"-m\", \"src.model_training.task\"],\n",
    "            \"args\": [\n",
    "                f'--train-data-file-pattern={TRANSFORM_TRAIN_DATA}',\n",
    "                f'--eval-data-file-pattern={TRANSFORM_EVAL_DATA}',\n",
    "                f'--nvt-workflow-dir={TRANSFORM_WORKFLOW_DIR}',\n",
    "                f'--num-epochs={NUM_EPOCHS}',\n",
    "                f'--learning-rate={LEARNING_RATE}',\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8fbbae",
   "metadata": {},
   "source": [
    "### Submit and monitor the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbd59fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = \"movielens-tf-training-{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "job = vertex_ai.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    ")\n",
    "\n",
    "job.run(\n",
    "    sync=True, \n",
    "    service_account=VERTEX_SERVICE_ACCOUNT,\n",
    "    tensorboard=TENSORBOARD_RESOURCE_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd846c33",
   "metadata": {},
   "source": [
    "## 6. Upload the model Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd43517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRITON_SERVER_IMAGE = f\"gcr.io/{PROJECT}/triton-{MODEL_DISPLAY_NAME}:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51eb980",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.Model.upload(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    artifact_uri=EXPORT_DIR,\n",
    "    serving_container_image_uri=TRITON_SERVER_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ccb153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedb0b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8c07919",
   "metadata": {},
   "source": [
    "# 02 - Experimentation\n",
    "\n",
    "This notebook covers the following steps:\n",
    "\n",
    "1. Preparing the data using `NVTabular` locally.\n",
    "2. Train, evaluate, and export a `TensorFlow` model locally.\n",
    "3. Build the Docker container image.\n",
    "4. Submit a `Vertex AI` custom job to prepare the data.\n",
    "5. Submit a `Vertex AI` custom job to train and export the model.\n",
    "6. Upload the exported model as a Vertex AI model resource."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33776e1a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec1ef3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\n",
    "\n",
    "from google.protobuf.internal import api_implementation\n",
    "api_implementation._implementation_type = 'python'\n",
    "print(api_implementation.Type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d035804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import cudf\n",
    "import nvtabular as nvt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from src.common import features, utils\n",
    "from src.data_preprocessing import etl\n",
    "from src.model_training import trainer\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f111da5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'merlin-on-gcp'\n",
    "REGION = 'us-central1'\n",
    "BUCKET = 'merlin-on-gcp'\n",
    "VERTEX_SERVICE_ACCOUNT = f'vertex-sa-mlops@{PROJECT}.iam.gserviceaccount.com'\n",
    "\n",
    "MOVIES_DATASET_DISPLAY_NAME = 'movielens25m-movies'\n",
    "RATINGS_DATASET_DISPLAY_NAME = 'movielens25m-ratings'\n",
    "\n",
    "MODEL_DISPLAY_NAME = f'movielens25m-recommender'\n",
    "\n",
    "WORKSPACE = f\"gs://{BUCKET}/movielens25m\"\n",
    "EXPERIMENT_ARTIFACTS_DIR = os.path.join(WORKSPACE, 'experiments')\n",
    "\n",
    "TENSORBOARD_DISPLAY_NAME = f'tb-{PROJECT}'\n",
    "EXPERIMENT_NAME = f'{MODEL_DISPLAY_NAME}-experiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e752c2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=BUCKET,\n",
    "    experiment=EXPERIMENT_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea4d12f",
   "metadata": {},
   "source": [
    "## Create Vertex TensorBoard Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0c4017",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud beta ai tensorboards create --display-name={TENSORBOARD_DISPLAY_NAME} \\\n",
    "  --project={PROJECT} --region={REGION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890e7885",
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSORBOARD_RESOURCE_NAME = \"projects/659831510405/locations/us-central1/tensorboards/4450717516120981504\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7b6091",
   "metadata": {},
   "source": [
    "## Initialize Vertex AI Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faef0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_EXPERIMENT_ARTIFACTS = False\n",
    "if tf.io.gfile.exists(EXPERIMENT_ARTIFACTS_DIR) and REMOVE_EXPERIMENT_ARTIFACTS:\n",
    "    print(\"Removing previous experiment artifacts...\")\n",
    "    tf.io.gfile.rmtree(EXPERIMENT_ARTIFACTS_DIR)\n",
    "\n",
    "if not tf.io.gfile.exists(EXPERIMENT_ARTIFACTS_DIR):\n",
    "    print(\"Creating new experiment artifacts directory...\")\n",
    "    tf.io.gfile.mkdir(EXPERIMENT_ARTIFACTS_DIR)\n",
    "\n",
    "print(\"Workspace is ready.\")\n",
    "\n",
    "run_id = f\"run-local-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "vertex_ai.start_run(run_id)\n",
    "\n",
    "EXPERIMENT_RUN_DIR = os.path.join(EXPERIMENT_ARTIFACTS_DIR, EXPERIMENT_NAME, run_id)\n",
    "print(\"Experiment run directory:\", EXPERIMENT_RUN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acf03d2",
   "metadata": {},
   "source": [
    "## 1. Preparing the data using NVTabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f2b051",
   "metadata": {},
   "outputs": [],
   "source": [
    "ETL_OUTPUT_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'etl_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44352709",
   "metadata": {},
   "outputs": [],
   "source": [
    "etl.run_etl(\n",
    "    PROJECT, \n",
    "    REGION, \n",
    "    MOVIES_DATASET_DISPLAY_NAME, \n",
    "    RATINGS_DATASET_DISPLAY_NAME, \n",
    "    ETL_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa28dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {ETL_OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453c0f95",
   "metadata": {},
   "source": [
    "## 2. Train a TensorFlow model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab73940c",
   "metadata": {},
   "source": [
    "### Prepare experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe750ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'logs')\n",
    "EXPORT_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef0a7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_params = {\n",
    "    'train_data_file_pattern': os.path.join(ETL_OUTPUT_DIR, 'transformed_data', 'train', '*.parquet'),\n",
    "    'test_data_file_pattern': os.path.join(ETL_OUTPUT_DIR, 'transformed_data', 'test', '*.parquet'),\n",
    "    'transform_workflow_dir': os.path.join(ETL_OUTPUT_DIR, 'transform_workflow'),\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 1024 * 32,\n",
    "    'hidden_units': [128, 128],\n",
    "    'num_epochs': 1\n",
    "}\n",
    "\n",
    "vertex_ai.log_params(experiment_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5cbd73",
   "metadata": {},
   "source": [
    "### Download the data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ada90c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.io.gfile.exists('data'):\n",
    "    tf.io.gfile.rmtree('data')\n",
    "if tf.io.gfile.exists('transform_workflow'):\n",
    "    tf.io.gfile.rmtree('transform_workflow')\n",
    "\n",
    "tf.io.gfile.mkdir('data')\n",
    "tf.io.gfile.mkdir('data/train')\n",
    "tf.io.gfile.mkdir('data/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c596145",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.copy_files(experiment_params['train_data_files'], 'data/train')\n",
    "utils.copy_files(experiment_params['test_data_files'], 'data/test')\n",
    "utils.download_directory(experiment_params['transform_workflow_dir'], '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1df94c",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e198cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_model = trainer.train(\n",
    "    train_data_file_pattern='data/train/*.parquet',\n",
    "    nvt_workflow_dir='transform_workflow',\n",
    "    hyperparams=experiment_params,\n",
    "    log_dir=LOG_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220e9bd9",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c875b7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loss, eval_mse = trainer.evaluate(\n",
    "    recommendation_model,\n",
    "    eval_data_file_pattern='data/test/*.parquet',\n",
    "    hyperparams=hyperparams\n",
    ")\n",
    "\n",
    "vertex_ai.log_metrics({\"val_loss\": eval_loss, \"eval_mae\": eval_mae})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5f742b",
   "metadata": {},
   "source": [
    "### Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dc0a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "\n",
    "imp.reload(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64214f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.export(\n",
    "    model=recommendation_model,\n",
    "    nvt_workflow=nvt.Workflow.load('transform_workflow'),\n",
    "    model_name=MODEL_DISPLAY_NAME,\n",
    "    export_dir=EXPORT_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004d04c0",
   "metadata": {},
   "source": [
    "## 3. Build training container image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10715f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME=\"nvt-cuda11.0-tf2.4\"\n",
    "IMAGE_URI=f\"gcr.io/{PROJECT}/{IMAGE_NAME}\"\n",
    "print(IMAGE_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f7adec",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud builds submit --tag {IMAGE_URI} . --timeout=30m --machine-type=e2-highcpu-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c821a82b",
   "metadata": {},
   "source": [
    "## 4. Submit Vertex AI Custom Job for ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2583b2",
   "metadata": {},
   "source": [
    "### Prepare worker pool specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393b540b",
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_pool_specs =  [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\",\n",
    "            \"accelerator_type\": \"NVIDIA_TESLA_V100\",\n",
    "            \"accelerator_count\": 1,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": IMAGE_URI,\n",
    "            \"command\": [\"python\", \"src/data_preprocessing/task.py\"],\n",
    "            \"args\": [\n",
    "                f'--project={PROJECT}', \n",
    "                f'--region={REGION}',\n",
    "                f'--movies-dataset-display=name='{MOVIES_DATASET_DISPLAY_NAME}',\n",
    "                f'--ratings-dataset-display=name='{RATINGS_DATASET_DISPLAY_NAME}',\n",
    "                f'--etl-output-dir='{ETL_OUTPUT_DIR}',\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c2cfaa",
   "metadata": {},
   "source": [
    "\n",
    "### Submit and monitor the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced0f6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = \"movielens-nvt-etl-{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "job = vertex_ai.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    ")\n",
    "\n",
    "job.run(\n",
    "    sync=True, \n",
    "    service_account=VERTEX_SERVICE_ACCOUNT,\n",
    "    tensorboard=TENSORBOARD_RESOURCE_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590ed9aa",
   "metadata": {},
   "source": [
    "## 5. Submit Vertex AI Custom Job for Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cc1217",
   "metadata": {},
   "source": [
    "### Prepare worker pool specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b0de54",
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_pool_specs =  [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\",\n",
    "            \"accelerator_type\": \"NVIDIA_TESLA_V100\",\n",
    "            \"accelerator_count\": 1,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": IMAGE_URI,\n",
    "            \"command\": [\"python\", \"src/data_preprocessing/task.py\"],\n",
    "            \"args\": [\n",
    "                f'--project={PROJECT}', \n",
    "                f'--region={REGION}',\n",
    "                f'--movies-dataset-display=name='{MOVIES_DATASET_DISPLAY_NAME}',\n",
    "                f'--ratings-dataset-display=name='{RATINGS_DATASET_DISPLAY_NAME}',\n",
    "                f'--etl-output-dir='{ETL_OUTPUT_DIR}',\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9657d66",
   "metadata": {},
   "source": [
    "### Submit and monitor the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cc1a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = \"movielens-tf-training-{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "job = vertex_ai.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    ")\n",
    "\n",
    "job.run(\n",
    "    sync=True, \n",
    "    service_account=VERTEX_SERVICE_ACCOUNT,\n",
    "    tensorboard=TENSORBOARD_RESOURCE_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e81aff",
   "metadata": {},
   "source": [
    "## 6. Upload the model Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e624427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRITON_SERVER_IMAGE = f\"gcr.io/{PROJECT}/triton-{MODEL_DISPLAY_NAME}:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7074855",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.Model.upload(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    artifact_uri=EXPORT_DIR,\n",
    "    serving_container_image_uri=TRITON_SERVER_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11755054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f9d0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m71"
  },
  "kernelspec": {
   "display_name": "nvt-11-0",
   "language": "python",
   "name": "nvt-11-0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

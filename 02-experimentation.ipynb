{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8320e92f",
   "metadata": {},
   "source": [
    "# 02 - Experimentation\n",
    "\n",
    "This notebook covers the following steps:\n",
    "\n",
    "1. Preparing the data using `NVTabular` locally.\n",
    "2. Train and export a `TensorFlow` model locally.\n",
    "3. Build the Docker container image.\n",
    "4. Submit a `Vertex AI` custom job to prepare the data.\n",
    "5. Submit a `Vertex AI` custom job to train and export the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234e6549",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5738f12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import cudf\n",
    "import nvtabular as nvt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from src.common import features, utils\n",
    "from src.data_preprocessing import etl\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532274c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'merlin-on-gcp'\n",
    "REGION = 'us-central1'\n",
    "BUCKET = 'merlin-on-gcp'\n",
    "VERTEX_SERVICE_ACCOUNT = f'vertex-sa-mlops@{PROJECT}.iam.gserviceaccount.com'\n",
    "\n",
    "MOVIES_DATASET_DISPLAY_NAME = 'movielens25m-movies'\n",
    "RATINGS_DATASET_DISPLAY_NAME = 'movielens25m-ratings'\n",
    "\n",
    "MODEL_DISPLAY_NAME = f'movielens25m-recommender'\n",
    "\n",
    "WORKSPACE = f\"gs://{BUCKET}/movielens25m\"\n",
    "EXPERIMENT_ARTIFACTS_DIR = os.path.join(WORKSPACE, 'experiments')\n",
    "\n",
    "TENSORBOARD_DISPLAY_NAME = f'tb-{PROJECT}'\n",
    "EXPERIMENT_NAME = f'{MODEL_DISPLAY_NAME}-experiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319a68ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=BUCKET,\n",
    "    experiment=EXPERIMENT_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832f47fa",
   "metadata": {},
   "source": [
    "## Create Vertex TensorBoard Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d5935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud beta ai tensorboards create --display-name={TENSORBOARD_DISPLAY_NAME} \\\n",
    "  --project={PROJECT} --region={REGION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ad88ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSORBOARD_RESOURCE_NAME = \"projects/659831510405/locations/us-central1/tensorboards/4450717516120981504\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929b71fa",
   "metadata": {},
   "source": [
    "## Initialize Vertex AI Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97488661",
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_EXPERIMENT_ARTIFACTS = False\n",
    "if tf.io.gfile.exists(EXPERIMENT_ARTIFACTS_DIR) and REMOVE_EXPERIMENT_ARTIFACTS:\n",
    "    print(\"Removing previous experiment artifacts...\")\n",
    "    tf.io.gfile.rmtree(EXPERIMENT_ARTIFACTS_DIR)\n",
    "\n",
    "if not tf.io.gfile.exists(EXPERIMENT_ARTIFACTS_DIR):\n",
    "    print(\"Creating new experiment artifacts directory...\")\n",
    "    tf.io.gfile.mkdir(EXPERIMENT_ARTIFACTS_DIR)\n",
    "\n",
    "print(\"Workspace is ready.\")\n",
    "\n",
    "run_id = f\"run-local-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "vertex_ai.start_run(run_id)\n",
    "\n",
    "EXPERIMENT_RUN_DIR = os.path.join(EXPERIMENT_ARTIFACTS_DIR, EXPERIMENT_NAME, run_id)\n",
    "print(\"Experiment run directory:\", EXPERIMENT_RUN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0b07b7",
   "metadata": {},
   "source": [
    "## 1. Preparing the data using NVTabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a96c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ETL_OUTPUT_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'etl_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688da19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "etl.run_etl(\n",
    "    PROJECT, \n",
    "    REGION, \n",
    "    MOVIES_DATASET_DISPLAY_NAME, \n",
    "    RATINGS_DATASET_DISPLAY_NAME, \n",
    "    ETL_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe66ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {ETL_OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3808bfe",
   "metadata": {},
   "source": [
    "## 2. Train a TensorFlow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdac6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'logs')\n",
    "EXPORT_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b628dd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_inputs = {\n",
    "    'train_data_files': os.path.join(ETL_OUTPUT_DIR, 'transformed_data', 'train', '*.parquet'),\n",
    "    'test_data_files': os.path.join(ETL_OUTPUT_DIR, 'transformed_data', 'test', '*.parquet'),\n",
    "    'transform_workflow_dir': os.path.join(ETL_OUTPUT_DIR, 'transform_workflow'),\n",
    "}\n",
    "\n",
    "hyperparams = {\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 1024 * 32,\n",
    "    'hidden_units': [128, 128],\n",
    "    'num_epochs': 1\n",
    "}\n",
    "\n",
    "vertex_ai.log_params(experiment_inputs)\n",
    "vertex_ai.log_params(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd8f22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "from src.model_training import trainer, model\n",
    "from src.common import utils, features\n",
    "\n",
    "imp.reload(trainer)\n",
    "imp.reload(features)\n",
    "imp.reload(utils)\n",
    "imp.reload(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41698da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.io.gfile.exists('data'):\n",
    "    tf.io.gfile.rmtree('data')\n",
    "if tf.io.gfile.exists('transform_workflow'):\n",
    "    tf.io.gfile.rmtree('transform_workflow')\n",
    "\n",
    "tf.io.gfile.mkdir('data')\n",
    "tf.io.gfile.mkdir('data/train')\n",
    "tf.io.gfile.mkdir('data/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9226e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.copy_files(experiment_inputs['train_data_files'], 'data/train')\n",
    "utils.copy_files(experiment_inputs['test_data_files'], 'data/test')\n",
    "utils.download_directory(experiment_inputs['transform_workflow_dir'], '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4babd054",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_model = trainer.train(\n",
    "    train_data_file_pattern='data/train/*.parquet',\n",
    "    nvt_workflow_dir='transform_workflow',\n",
    "    hyperparams=hyperparams,\n",
    "    log_dir=LOG_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8462de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metric = trainer.evaluate(\n",
    "    recommendation_model,\n",
    "    eval_data_file_pattern='data/test/*.parquet',\n",
    "    hyperparams=hyperparams\n",
    ")\n",
    "\n",
    "evaluation_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b74ef62",
   "metadata": {},
   "source": [
    "## 3. Build training container image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a125dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME=\"nvt-cuda11.0-tf2.4\"\n",
    "IMAGE_URI=f\"gcr.io/{PROJECT}/{IMAGE_NAME}\"\n",
    "print(IMAGE_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db6198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud builds submit --tag {IMAGE_URI} . --timeout=30m --machine-type=e2-highcpu-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101359ce",
   "metadata": {},
   "source": [
    "## 4. Submit Vertex AI Custom Job for ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fad5e7",
   "metadata": {},
   "source": [
    "### Prepare worker pool specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3600a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_pool_specs =  [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\",\n",
    "            \"accelerator_type\": \"NVIDIA_TESLA_V100\",\n",
    "            \"accelerator_count\": 1,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": IMAGE_URI,\n",
    "            \"command\": [\"python\", \"src/data_preprocessing/task.py\"],\n",
    "            \"args\": [\n",
    "                f'--project={PROJECT}', \n",
    "                f'--region={REGION}',\n",
    "                f'--movies-dataset-display=name='{MOVIES_DATASET_DISPLAY_NAME}',\n",
    "                f'--ratings-dataset-display=name='{RATINGS_DATASET_DISPLAY_NAME}',\n",
    "                f'--etl-output-dir='{ETL_OUTPUT_DIR}',\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a91228f",
   "metadata": {},
   "source": [
    "\n",
    "### Submit and monitor the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a6c3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = \"movielens-nvt-etl-{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "job = vertex_ai.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    ")\n",
    "\n",
    "job.run(\n",
    "    sync=True, \n",
    "    service_account=VERTEX_SERVICE_ACCOUNT,\n",
    "    tensorboard=TENSORBOARD_RESOURCE_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214a1edc",
   "metadata": {},
   "source": [
    "## 5. Submit Vertex AI Custom Job for Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5054559",
   "metadata": {},
   "source": [
    "### Prepare worker pool specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e886c50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_pool_specs =  [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\",\n",
    "            \"accelerator_type\": \"NVIDIA_TESLA_V100\",\n",
    "            \"accelerator_count\": 1,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": IMAGE_URI,\n",
    "            \"command\": [\"python\", \"src/data_preprocessing/task.py\"],\n",
    "            \"args\": [\n",
    "                f'--project={PROJECT}', \n",
    "                f'--region={REGION}',\n",
    "                f'--movies-dataset-display=name='{MOVIES_DATASET_DISPLAY_NAME}',\n",
    "                f'--ratings-dataset-display=name='{RATINGS_DATASET_DISPLAY_NAME}',\n",
    "                f'--etl-output-dir='{ETL_OUTPUT_DIR}',\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba92528",
   "metadata": {},
   "source": [
    "### Submit and monitor the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0327fcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = \"movielens-tf-training-{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "job = vertex_ai.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    ")\n",
    "\n",
    "job.run(\n",
    "    sync=True, \n",
    "    service_account=VERTEX_SERVICE_ACCOUNT,\n",
    "    tensorboard=TENSORBOARD_RESOURCE_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552ea1cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49eba16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432197d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m71"
  },
  "kernelspec": {
   "display_name": "nvt-11-0",
   "language": "python",
   "name": "nvt-11-0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
